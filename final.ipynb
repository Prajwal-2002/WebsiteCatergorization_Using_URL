{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import spacy as sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('C:/Users/RIDHIMAN/Desktop/refined_ds.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[['website_url','cleaned_data','Category']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.Category.unique()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_id'] = df['Category'].factorize()[0]\n",
    "category_id_df = df[['Category', 'category_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Category']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(df.cleaned_data.values.astype('U')).toarray()\n",
    "labels = df.category_id\n",
    "print(\"Each of the %d text is represented by %d features\" %(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"\\n==> %s:\" %(Category))\n",
    "  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
    "  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_data'] \n",
    "y = df['Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n",
    "                                                               labels,\n",
    "                                                               df.index, test_size=0.15,shuffle=True, \n",
    "                                                               random_state=0)\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "calibrated_svc = CalibratedClassifierCV(base_estimator=model,\n",
    "                                        cv=\"prefit\")\n",
    "\n",
    "calibrated_svc.fit(X_train,y_train)\n",
    "predicted = calibrated_svc.predict(X_test)\n",
    "print(\"acc:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, predicted,labels=[0,1,2,3])\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"OrRd\", fmt='d',\n",
    "            xticklabels=category_id_df.Category.values, \n",
    "            yticklabels=category_id_df.Category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for predicted in category_id_df.category_id:\n",
    "    for actual in category_id_df.category_id:\n",
    "        if predicted != actual and conf_mat[actual, predicted] >0:\n",
    "            print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual],id_to_category[predicted],\n",
    "                                                                   conf_mat[actual, predicted]))\n",
    "            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Category', \n",
    "                                                                'cleaned_data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(features, labels)\n",
    "\n",
    "N = 4\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  indices = np.argsort(model.coef_[category_id])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "  print(\"\\n==> '{}':\".format(Category))\n",
    "  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n",
    "  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['category_id'], \n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "m = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)\n",
    "m1=CalibratedClassifierCV(base_estimator=m,\n",
    "                                        cv=\"prefit\").fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4 as bs4\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from collections import Counter\n",
    "import os\n",
    "class ScrapingTool:\n",
    "    def visit_url(self, website_url):\n",
    "        content = requests.get(website_url,timeout=60).content\n",
    "        \n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        result = {\n",
    "            \"website_url\": website_url,\n",
    "            \"website_name\": self.get_website_name(website_url),\n",
    "            \"website_text\": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+\n",
    "                                                               self.get_text_content(soup)\n",
    "        }\n",
    "        \n",
    "        return pd.Series(result)\n",
    "    \n",
    "    def get_website_name(self,website_url):\n",
    "        return \"\".join(urlparse(website_url).netloc.split(\".\")[-2])\n",
    "    \n",
    "    def get_html_title_tag(self,soup):\n",
    "        return '. '.join(soup.title.contents)\n",
    "    \n",
    "    def get_html_meta_tags(self,soup):\n",
    "        tags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n",
    "        content = [str(tag[\"content\"]) for tag in tags if tag[\"name\"] in ['keywords','description']]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_html_heading_tags(self,soup):\n",
    "        tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n",
    "        content = [\" \".join(tag.stripped_strings) for tag in tags]\n",
    "        return ' '.join(content)\n",
    "    \n",
    "    def get_text_content(self,soup):\n",
    "        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n",
    "        tags = soup.find_all(text=True)\n",
    "        result = []\n",
    "        for tag in tags:\n",
    "            stripped_tag = tag.strip()\n",
    "            if tag.parent.name not in tags_to_ignore\\\n",
    "                and isinstance(tag, bs4.element.Comment)==False\\\n",
    "                and not stripped_tag.isnumeric()\\\n",
    "                and len(stripped_tag)>0:\n",
    "                result.append(stripped_tag)\n",
    "        return ' '.join(result)\n",
    "\n",
    "\n",
    "sp.prefer_gpu()\n",
    "nlp = sp.load('en_core_web_sm')\n",
    "import re\n",
    "def clean_text(doc):\n",
    "    doc = nlp(doc)\n",
    "    tokens = []\n",
    "    exclusion_list = [\"nan\"]\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum()==False) or token.text in exclusion_list :\n",
    "            continue\n",
    "        token = str(token.lemma_.lower().strip())\n",
    "        tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('C:/Users/RIDHIMAN/Desktop/dish_dataset_gen.csv')\n",
    "dataframe.shape\n",
    "df = dataframe[['website']].copy()\n",
    "df = df.dropna()\n",
    "lst = []\n",
    "for i in range(0,dataframe.shape[0]):\n",
    "  lst.append(dataframe.website.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  try:\n",
    "    for i in lst:\n",
    "      website = i\n",
    "      scrapTool = ScrapingTool()\n",
    "      web=dict(scrapTool.visit_url(website))\n",
    "      text=(clean_text(web['website_text']))\n",
    "      t=fitted_vectorizer.transform([text])\n",
    "      print(website, id_to_category[m1.predict(t)[0]])\n",
    "    \n",
    "  except:\n",
    "    print(i+\"\\n\\n\\n\\n\\n\")\n",
    "    lst.remove(i)\n",
    "  else:\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
